{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling with /usr/local/bin/clang-omp\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "sys.path.append('src/util')\n",
    "\n",
    "import set_compiler\n",
    "set_compiler.install()\n",
    "\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sys.path.append('src')\n",
    "from OnlineVariationalInference.ovi import oviLDA\n",
    "from CollapsedGibbsSampler.cgs import cgsLDA\n",
    "from LDAutil import Evaluation\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "More and more text contents are shared on internet: news article, blog posts, user reviews... All these texts carry relevant information but the issue is how to process them. They could be used in machine learning models to refine prediction. \n",
    "\n",
    "For example, one popular application are the recommender systems. The goal is here to find what the users are more likely to like based on what the people similar to them liked. If we consider movies, we would like to use the synopsis, for products on online stores, we would like to use their descriptions. We could represent the text with its word counts in a [document, term] matrix. But this representation do not depict the underlying structure of the documents and their inner relationships.\n",
    "\n",
    "One possible solution to this problem is to apply a Latent Dirichlet Allocation, a topic modeling algorithm to find the latent features of the documents. Unfortunately, these methods requires many computations for large datasets. There is one serial implementation in python (https://pypi.python.org/pypi/lda) which already takes 50 minutes to run for a small document term matrix of size (2000 docs, 10 000 words).\n",
    "\n",
    "The goal of our project is to implement an LDA package in Python implementing two inference methods: the gibbs sampler and the online variatonnal inference. We took advantage of the Cython and Multithreading modules to build an optimized and parallel algorithm. Our deliverabe is be an open-source library, fully documented, easy to install and much faster than the current tools.\n",
    "\n",
    "To tackle this problem, we have to deal on one hand with ninja skills in Cython and parallel design to come up with a highly efficient algorithm and on the other hand with deep knowledge of software engineering to bring the different methods together into a functionnal package.\n",
    "\n",
    "In this report, we first provide the theoretical background needed to grasp the algorithm and explain the data we consider to test our algorithm. Then, we present the design of our package, digg deeply in the algorithm implementation and provides our performance results, in terms both of execution time and model accuracy. Lastly, we expose the possible improvements for the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Theory\n",
    "\n",
    "### 1.1 LDA Model\n",
    "\n",
    "The LDA is a three-level hierarchical Bayesian model. The basic idea is that documents are represented as random mixtures over latent topics and each topic is characterized by a distribution over words.\n",
    "\n",
    "The generative process for a document w in a corpus D is the following:\n",
    "\n",
    "1. Choose the topics representation: $\\phi \\sim$ Dir($\\beta$)\n",
    "* Choose the number of words: $ N \\sim $ Poisson($ \\xi $)\n",
    "* Choose the distribution of topics: $\\theta_w \\sim$ Dir($\\alpha_w$)\n",
    "* For each of the N words:\n",
    "    1. Choose a topic assignement: $z_{n,w} \\sim \\text{Multinomial}(\\theta_w)$\n",
    "\t* Choose a word: $w_n \\sim \\text{ Multinomial}(\\phi_{z_{n,w}})$\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Inference methods\n",
    "\n",
    "The main issue relies in computing the posterior distribution of the hidden variables given a document:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\theta, \\mathbf{z} |\\mathbf{w}, \\alpha, \\beta) = \\frac{p(\\theta, \\mathbf{z}, \\mathbf{w} | \\alpha, \\beta)}{p(\\mathbf{w} | \\alpha, \\beta)}\n",
    "\\end{align*}\n",
    "\n",
    "This distribution is intractable. As mentioned above, we used two  methods to infer it: through a Gibbs sampler or variational inference. In the first case, we are estimating the hyper parameters $\\theta$ and $\\phi$ with samples on the different variables. Alternatively, we chose the online variational inference method which finds the variational parameters that optimize a lower bound on the loglikelihood. \n",
    "\n",
    "##### 1.2.1 Gibbs sampling\n",
    "\n",
    "##### 1.2.2 Online Variational Inference\n",
    "\n",
    "The setup of the Variational Inference is as follows. We first approximate the true posterior by a simpler and factorised distribution:\n",
    "$$q(\\boldsymbol{z},\\boldsymbol{\\theta},\\boldsymbol{\\beta}) = q(\\boldsymbol{z})q(\\boldsymbol{\\theta})q(\\boldsymbol{\\beta}) $$ \n",
    "\n",
    "where:\n",
    "$$ q(z_{di} = k) = \\phi_{d_{w_{di}k}} \\quad q(\\theta_d) = \\text{Dir}(\\theta_d,\\gamma_d) \\quad q(\\beta_k) = \\text{Dir}(\\beta_k,\\lambda_k) $$\n",
    "\n",
    "The $\\boldsymbol{\\gamma}$ parameter rules the topic assignments for each document and the $\\boldsymbol{\\lambda}$, the topics themselves. We then minimise the KL divergence between the distribution $q$ and the true posterior $p$ like for the usual variational inference. What differs in online variational inference is that we sample a batch of document at each step and perform the E-step as if this batch constituted the entire corpus. \n",
    "\n",
    "We present the algorithm below:\n",
    "\n",
    "<img src=\"img/ovi_pseudocode.png\" width=500 height=300/>\n",
    "\n",
    "where $\\kappa \\in (0.5,1]$ rules how fast we forget old values of $\\tilde{\\lambda}$ and $\\tau_{0}$ how much weight one wants to put on the first iterations. \n",
    "\n",
    "\n",
    "### 1.3 Evaluation methods\n",
    "\n",
    "We need a measure to evaluate the performance of our model and to tune the hyperparameters. We use perplexity on held-out data as a measure of our model fit. Perplexity is defined as the geometric mean of the log likelihood of the words in the held-out set of documents given the trained model. In our case, for each document we held out 20% of the words which constitue the test set.\n",
    "\n",
    "\\begin{align*}\n",
    "\tperplexity(D_{test}) & = \\frac{\\sum\\limits_{d \\in D_{test}} \\log p(words)}{\\sum\\limits_{d \\in D_{test}|d|}}\\\\\n",
    "\tperplexity(D_{test}) & = \\frac{\\sum\\limits_{d \\in D_{test}} \\sum\\limits_{w \\in d} \\log \\left( \\sum_{t \\in topics} p(w|t)p(t|d) \\right)}{\\sum\\limits_{d \\in D_{test}|d|}}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data\n",
    "\n",
    "### 2.1 Presentation\n",
    "\n",
    "We chose to apply the latent dirichlet allocation on user reviews from the Yelp website. We focused only on the restaurants from a specific city (Las Vegas). The objective is to infer both the restaurants categories present in Las Vegas (using the topic distribution provided by the lda) and the categories for each restaurant (using the topics assignement of eahc restaurant).\n",
    "\n",
    "We download the raw data made available by Yelp for its dataset challenge competition (http://www.yelp.com/dataset_challenge). For the Las Vegas restaurants, we have ~20 000 reviews of average size ~100 words, the set of unique words being larger than 50 000.\n",
    "\n",
    "\n",
    "### 2.2 Processing\n",
    "\n",
    "The dataset contains 3822 businesses, considered here as documents. Cleaning the reviews represented a significant part of this final project. We present some of the steps we followed in the process:\\\\\n",
    "\n",
    "* We aggregated all the reviews about a particular business into a \"super\" review, in order to get a general sense of what the restaurant was about;\n",
    "* We applied spelling corrections, removed common stopwords and generic words such as \"table\" or \"restaurants\";\n",
    "* We attempted at only selecting common words and not adjectives in order to focus on what the venues were about and not the users' opinions (this process was done taking advantage of Spark to reduce the computing time);\n",
    "* We transformed the corpus of \"super reviews\" into a document-term matrix that we could exploit.\n",
    "* We aggregated reviews so some words may be much more used than other and we still want to take advantage of all the available information. As a result, we down scaled the words counts into the range [0, 100]. \n",
    "\n",
    "The processing lead to a document term matrix of size (2 000 words, 10 000 documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Package design\n",
    "\n",
    "We worked hard on building a very well organized and documentation package. The documentation is provided in the readme file on the github repository : https://github.com/virgodi/plda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Implementation Details\n",
    "\n",
    "### 4.1 OVI\n",
    "\n",
    "The online variationnal inference implementation implies a single pass of the algorithm on each document. The only parallelisation is feasible on the loop over the documents is using mini-batch of documents and updating the topics distribution between these batch. As a result, we chose to implement a mini-batch version where the work inside the batch is divided among threads. \n",
    "\n",
    "We first wrote a functionnal version in Cython without using any Python library (like numpy) to be able to release the gil. To parallelise the work among threads, we needed to find an efficient combination of shared and private memories between the threads.\n",
    "\n",
    "#### Cython code\n",
    "\n",
    "memory view type used, taylor approximation of the digamma function\n",
    "\n",
    "#### Multithreading\n",
    "\n",
    "local version per thread of the temporary topics distribution. Less expensive than locks . Not possible to work on the same topics matrix with locks over words region because of the gamma update at each iteration of the inner loop, neither it is with locks over the topics because of the normalization needed to update gamma.\n",
    "\n",
    "\n",
    "### 4.2 GS\n",
    "\n",
    "details of the pseudo code, cython use and multithreading (with 2 different approaches on lock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "Running a cross validation on the ideal number of topics with the ovi, show plot of perplexity \n",
    "\n",
    "Copy and paste a test script building LDA on the las vegas dataset with different number of threads and locks approach for gs. Plotting the time over the number of threads for OVI and for the 2 locking of GS (3 plots).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the data: too heavy to be on github\n",
    "# Be sure to have them on local\n",
    "dtm = np.load('../lv_dtm.npy').astype(int)\n",
    "vocablv = np.load('../Lasvegas/lv_vocab10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.99 s, sys: 383 ms, total: 9.38 s\n",
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "# Cross validation on the number of topics\n",
    "K = range(40, 80, 3)\n",
    "model_ovi = oviLDA(num_topics=10, num_threads=8, batch_size=50)\n",
    "perplexity_dict = {}\n",
    "for k in K:\n",
    "    model_ovi.set_topics(k)\n",
    "    %time model_ovi.fit(dtm)\n",
    "    perplexity_dict[k] = model_ovi.perplexity_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEZCAYAAABWwhjiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRMzMrLD/D0ZojjunZ/oGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e6e3c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perp_val = sorted(perplexity_dict.items(), key=lambda x: x[0])\n",
    "plt.plot([p[0] for p in perp_val], [p[1] for p in perp_val])\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Perplexity')\n",
    "\n",
    "plt.title('Cross validation on the number of topics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Learnings\n",
    "\n",
    "We need to present interestings insights we learned from the project and what we enjoyed the most..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
