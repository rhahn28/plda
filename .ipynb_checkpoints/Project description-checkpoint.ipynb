{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "More and more text contents are shared on internet: news article, blog posts, user reviews... All these texts carry relevant information but the issue is how to process them. They could be used in machine learning models to refine prediction. \n",
    "\n",
    "For example, one popular application are the recommender systems. The goal is here to find what the users are more likely to like based on what the people similar to them liked. If we consider movies, we would like to use the synopsis, for products on online stores, we would like to use their descriptions. We could represent the text with its word counts in a [document, term] matrix. But this representation do not depict the underlying structure of the documents and their inner relationships.\n",
    "\n",
    "One possible solution to this problem is to apply a Latent Dirichlet Allocation, a topic modeling algorithm to find the latent features of the documents. Unfortunately, these methods requires many computations for large datasets. There is one serial implementation in python (https://pypi.python.org/pypi/lda) which already takes 50 minutes to run for a small document term matrix of size (2000 docs, 10 000 words).\n",
    "\n",
    "The goal of our project is to implement an LDA package in Python implementing two inference methods: the gibbs sampler and the online variatonnal inference. We took advantage of the Cython and Multithreading modules to build an optimized and parallel algorithm. Our deliverabe is be an open-source library, fully documented, easy to install and much faster than the current tools.\n",
    "\n",
    "To tackle this problem, we have to deal on one hand with ninja skills in Cython and parallel design to come up with a highly efficient algorithm and on the other hand with deep knowledge of software engineering to bring the different methods together into a functionnal package.\n",
    "\n",
    "In this report, we first explain the data we consider to test our algorithm. Then, we present the design of our package, digg deeply in the algorithm implementation and provides our performance results, in terms both of execution time and model accuracy. Lastly, we expose the possible improvements for the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Theory\n",
    "\n",
    "### LDA Model\n",
    "\n",
    "The LDA is a three-level hierarchical Bayesian model. The basic idea is that documents are represented as random mixtures over latent topics and each topic is characterized by a distribution over words.\n",
    "\n",
    "1. Choose the topics representation: $\\phi \\sim$ Dir($\\beta$)\n",
    "* Choose the number of words: $ N \\sim $ Poisson($ \\xi $)\n",
    "* Choose the distribution of topics: $\\theta_w \\sim$ Dir($\\alpha_w$)\n",
    "* For each of the N words:\n",
    "    1. Choose a topic assignement: $z_{n,w} \\sim \\text{Multinomial}(\\theta_w)$\n",
    "\t* Choose a word: $w_n \\sim \\text{ Multinomial}(\\phi_{z_{n,w}})$\n",
    "\n",
    "\n",
    "The generative process for a document w in a corpus D is the following:\n",
    "\n",
    "###Inference methods\n",
    "\n",
    "Presenting global ideas and main equation \n",
    "\n",
    "##### Gibbs sampling\n",
    "\n",
    "##### Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Preprocessing\n",
    "\n",
    "obtention ==> Yelp dataset, reviews aggregate by restaurant\n",
    "conversion into dtm: cut for only frequent words, removing stop words, removing adjective, rescaling the distribution (because each reviews carry common words)\n",
    "\n",
    "bilan: numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Package design\n",
    "\n",
    "1 class: attributes, methods (to do in a separate documentation because also needed in the readme, well put the link here)\n",
    "\n",
    "test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Implementation\n",
    "\n",
    "### OVI\n",
    "\n",
    "details of the pseudo code, cython use and multithreading (present the impossibility to divide by region the words)\n",
    "\n",
    "### GS\n",
    "\n",
    "details of the pseudo cpde, cython use and multithreading (with 2 different approaches on lock\n",
    "\n",
    "### Result\n",
    "\n",
    "provide time results (explaining the comparaison between the two algo)+ topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Learnings\n",
    "\n",
    "We need to present interestings insights we learned from the project and what we enjoyed the most..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
